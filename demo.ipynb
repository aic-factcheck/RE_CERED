{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P0 other', 'P19 místo narození', 'P20 místo úmrtí', 'P21 pohlaví', 'P22 otec', 'P25 matka', 'P26 choť', 'P27 státní občanství', 'P36 hlavní město', 'P39 ve funkci', 'P47 hraničí s', 'P50 autor', 'P53 rodina', 'P54 člen sportovního družstva', 'P97 šlechtický titul', 'P101 zaměření', 'P102 stranická příslušnost', 'P105 taxonomické zařazení', 'P106 povolání', 'P118 liga', 'P131 nachází se v administrativní jednotce', 'P136 žánr', 'P137 operátor', 'P138 pojmenováno po', 'P140 vyznání', 'P150 nižší správní celky', 'P155 předchozí', 'P159 sídlo', 'P161 hraje', 'P171 nadřazený taxon', 'P175 interpret', 'P176 výrobce', 'P179 série', 'P206 u vodní plochy', 'P264 hudební vydavatelství', 'P276 místo', 'P279 nadtřída', 'P360 seznam (čeho)', 'P361 část (čeho)', 'P413 pozice hráče', 'P425 obor tohoto povolání', 'P460 údajně totéž co', 'P461 protiklad', 'P463 člen (čeho)', 'P495 země původu', 'P527 skládá se z', 'P530 diplomatický vztah', 'P641 sport', 'P669 ulice', 'P706 oblast', 'P708 diecéze', 'P734 příjmení', 'P735 rodné jméno', 'P740 místo vzniku', 'P800 dílo', 'P921 hlavní téma díla', 'P974 přítok', 'P1056 produkuje', 'P1303 hudební nástroj', 'P1376 hlavní sídlo čeho', 'P1383 zahrnuje sídlo', 'P1889 rozdílné od', 'P3373 sourozenec', 'P4552 pohoří']\n",
      "{0: 'P0 other', 1: 'P19 místo narození', 2: 'P20 místo úmrtí', 3: 'P21 pohlaví', 4: 'P22 otec', 5: 'P25 matka', 6: 'P26 choť', 7: 'P27 státní občanství', 8: 'P36 hlavní město', 9: 'P39 ve funkci', 10: 'P47 hraničí s', 11: 'P50 autor', 12: 'P53 rodina', 13: 'P54 člen sportovního družstva', 14: 'P97 šlechtický titul', 15: 'P101 zaměření', 16: 'P102 stranická příslušnost', 17: 'P105 taxonomické zařazení', 18: 'P106 povolání', 19: 'P118 liga', 20: 'P131 nachází se v administrativní jednotce', 21: 'P136 žánr', 22: 'P137 operátor', 23: 'P138 pojmenováno po', 24: 'P140 vyznání', 25: 'P150 nižší správní celky', 26: 'P155 předchozí', 27: 'P159 sídlo', 28: 'P161 hraje', 29: 'P171 nadřazený taxon', 30: 'P175 interpret', 31: 'P176 výrobce', 32: 'P179 série', 33: 'P206 u vodní plochy', 34: 'P264 hudební vydavatelství', 35: 'P276 místo', 36: 'P279 nadtřída', 37: 'P360 seznam (čeho)', 38: 'P361 část (čeho)', 39: 'P413 pozice hráče', 40: 'P425 obor tohoto povolání', 41: 'P460 údajně totéž co', 42: 'P461 protiklad', 43: 'P463 člen (čeho)', 44: 'P495 země původu', 45: 'P527 skládá se z', 46: 'P530 diplomatický vztah', 47: 'P641 sport', 48: 'P669 ulice', 49: 'P706 oblast', 50: 'P708 diecéze', 51: 'P734 příjmení', 52: 'P735 rodné jméno', 53: 'P740 místo vzniku', 54: 'P800 dílo', 55: 'P921 hlavní téma díla', 56: 'P974 přítok', 57: 'P1056 produkuje', 58: 'P1303 hudební nástroj', 59: 'P1376 hlavní sídlo čeho', 60: 'P1383 zahrnuje sídlo', 61: 'P1889 rozdílné od', 62: 'P3373 sourozenec', 63: 'P4552 pohoří'}\n",
      "{'P0 other': 0, 'P19 místo narození': 1, 'P20 místo úmrtí': 2, 'P21 pohlaví': 3, 'P22 otec': 4, 'P25 matka': 5, 'P26 choť': 6, 'P27 státní občanství': 7, 'P36 hlavní město': 8, 'P39 ve funkci': 9, 'P47 hraničí s': 10, 'P50 autor': 11, 'P53 rodina': 12, 'P54 člen sportovního družstva': 13, 'P97 šlechtický titul': 14, 'P101 zaměření': 15, 'P102 stranická příslušnost': 16, 'P105 taxonomické zařazení': 17, 'P106 povolání': 18, 'P118 liga': 19, 'P131 nachází se v administrativní jednotce': 20, 'P136 žánr': 21, 'P137 operátor': 22, 'P138 pojmenováno po': 23, 'P140 vyznání': 24, 'P150 nižší správní celky': 25, 'P155 předchozí': 26, 'P159 sídlo': 27, 'P161 hraje': 28, 'P171 nadřazený taxon': 29, 'P175 interpret': 30, 'P176 výrobce': 31, 'P179 série': 32, 'P206 u vodní plochy': 33, 'P264 hudební vydavatelství': 34, 'P276 místo': 35, 'P279 nadtřída': 36, 'P360 seznam (čeho)': 37, 'P361 část (čeho)': 38, 'P413 pozice hráče': 39, 'P425 obor tohoto povolání': 40, 'P460 údajně totéž co': 41, 'P461 protiklad': 42, 'P463 člen (čeho)': 43, 'P495 země původu': 44, 'P527 skládá se z': 45, 'P530 diplomatický vztah': 46, 'P641 sport': 47, 'P669 ulice': 48, 'P706 oblast': 49, 'P708 diecéze': 50, 'P734 příjmení': 51, 'P735 rodné jméno': 52, 'P740 místo vzniku': 53, 'P800 dílo': 54, 'P921 hlavní téma díla': 55, 'P974 přítok': 56, 'P1056 produkuje': 57, 'P1303 hudební nástroj': 58, 'P1376 hlavní sídlo čeho': 59, 'P1383 zahrnuje sídlo': 60, 'P1889 rozdílné od': 61, 'P3373 sourozenec': 62, 'P4552 pohoří': 63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at stulcrad/XLM-RoBERTa-2 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P19 místo narození\n",
      "P463 člen (čeho)\n",
      "P264 hudební vydavatelství\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from utils.RE_utils.CERED.RE_model_base import RelationExtractionModel\n",
    "from utils.RE_utils.CERED.RE_model_base import consts\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "labels_path = \"/home/stulcrad/ner-radek-stulc/utils/RE_utils/CERED/CERED2_LABELS\"\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "LABELS = []\n",
    "with open(labels_path, \"r\") as f:\n",
    "    LABELS = json.load(f)\n",
    "    LABELS = [x[0] + ' ' + x[1] for x in LABELS]\n",
    "    for i, label in enumerate(LABELS):\n",
    "        id2label[i] = label\n",
    "        label2id[label] = i\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stulcrad/XLM-RoBERTa-2\", add_prefix_space=True)\n",
    "model = RelationExtractionModel.from_pretrained(\"stulcrad/XLM-RoBERTa-2\", tokenizer=tokenizer,\n",
    "                                                id2label=id2label, label2id=label2id,\n",
    "                                                num_labels=len(LABELS))\n",
    "\n",
    "sentence1 = \"[E1] Jan [/E1] se narodil v [E2] Praze [/E2] .\"\n",
    "sentence2 = \"Vydáno bylo 17 . února roku 2017 společností [E2] Domino Records [/E2] , téměř přesně rok po vydání desky [E1] Painting With [/E1] . \"\n",
    "sentence_example = \"[E1] Jan Máchal [/E1] ( 9. února 1864, Třebíč [UNK] 15. října 1924, Třebíč ) byl český pedagog, činovník [E2] Sokola [/E2] . \"\n",
    "special_token_dict = {\n",
    "    '[E1]':consts.E1_START_TOKEN, '[/E1]':consts.E1_END_TOKEN,\n",
    "    '[E2]':consts.E2_START_TOKEN, '[/E2]':consts.E2_END_TOKEN\n",
    "}\n",
    "\n",
    "marked = [token if token not in special_token_dict else special_token_dict[token] for token in sentence1.split(' ')]\n",
    "marked2 = [token if token not in special_token_dict else special_token_dict[token] for token in sentence2.split(' ')]\n",
    "marked_example = [token if token not in special_token_dict else special_token_dict[token] for token in sentence_example.split(' ')]\n",
    "# print(marked)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "marked_sentence = \"\"\n",
    "marked_sentence_example = \"\"\n",
    "marked_sentence2 = \"\"\n",
    "first = True\n",
    "for token in marked:\n",
    "    if first:\n",
    "        first = False\n",
    "        marked_sentence += token\n",
    "    else:\n",
    "        marked_sentence += \" \" + token\n",
    "first = True\n",
    "for token in marked_example:\n",
    "    if first:\n",
    "        first = False\n",
    "        marked_sentence_example += token\n",
    "    else:\n",
    "        marked_sentence_example += \" \" + token\n",
    "first = True\n",
    "for token in marked2:\n",
    "    if first:\n",
    "        first = False\n",
    "        marked_sentence2 += token\n",
    "    else:\n",
    "        marked_sentence2 += \" \" + token\n",
    "\n",
    "\n",
    "inputs = tokenizer(marked_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "inputs_example = tokenizer(marked_sentence_example, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "inputs2 = tokenizer(marked_sentence2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "inputs.to(device)\n",
    "inputs_example.to(device)\n",
    "inputs2.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    outputs_example = model(**inputs_example)\n",
    "    outputs2 = model(**inputs2)\n",
    "    logits = outputs[\"logits\"]\n",
    "    logits_example = outputs_example[\"logits\"]\n",
    "    logits2 = outputs2[\"logits\"]\n",
    "\n",
    "pred_labels = logits.argmax().item()\n",
    "pred_labels_example = logits_example.argmax().item()\n",
    "pred_labels2 = logits2.argmax().item()\n",
    "print(id2label[pred_labels])\n",
    "print(id2label[pred_labels_example])\n",
    "print(id2label[pred_labels2])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
